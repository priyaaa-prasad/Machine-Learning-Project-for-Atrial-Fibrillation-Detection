# -*- coding: utf-8 -*-
"""ecg_classifier_priya.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/106n5YRBpXeNASoqSd3-3qbMqsB_WoI4l

## Import the modules we need
These are the common ones we've used previously
"""

import numpy as np
import tensorflow as tf
import keras
import matplotlib.pyplot as plt

"""## Mount your Google Drive to this Colab session
We are doing this so that we can read in data that is stored on your Google Drive (your ECG training data)

When you run the drive.mount() function, you will have to give access. Follow the prompts and log into your Google account...then you can access files in your Google Drive.  You only have to run this block once in a session
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""## Load in the ECG data from the file
Store the file in a list of lines in the file that we will process to get the ECG data.

*fname* is the location I stored my ECG data file.  You will have to change this to where you saved the ECG data on your Google Drive
"""

# Change fname to correct location
fname = '/content/gdrive/MyDrive/physionet2017_ECG_training_data.csv'
with open( fname ) as file:
  lines = file.readlines()

from google.colab import drive
drive.mount('/content/drive')

"""## Parse each line of ECG data
The last piece of data on each line is the label.

The rest of the data on each line is the ECG signal collected at 300 Hz.  The original data was of variable length, but for this project, each line contains exactly 2000 data points from the ECG.

### All we did here was take each of the 2000 data points and made them each a feature for our model.
*You may want to consider some pre-processing or data manipulation of the input features*
"""

# Set up a list of features and list of corresponding labels
features = []
labels = []
for line in lines:

# We know its a .csv file, so split each line apart by commas
    pieces = line.split(',')

# The last piece of data on each line is the label (0,1,2,3)
# We could convert it to integer data, but its OK to leave as strings because it
# will be converted to a one-hot vector anyway
    labels.append( pieces[-1].strip() )

# Split out the ECG data (don't include the last column)
# At this poitnt each piece is still treated as a string, so it has to be converted to a floating point number
    string_data=pieces[0:-1]

# Store all the data for each case in a list
    data = []
    for piece in string_data:
        data.append( float( piece ) )

# Store the ECG data as a tensor in the features list
    features.append( tf.convert_to_tensor( data ) )

"""## Look at the ECG data
Set EXAMPLE to some number which is the index of the ECG trace to look at
This snippet will plot the ECG trace and report the label
"""

EXAMPLE = 12
plt.figure()
plt.plot( features[EXAMPLE] )
plt.show()
print( labels[EXAMPLE] )

"""## Split the data up into a training set and a validation set
Had to use convert to tensor function to convert the input for the model from a list to a tensor.
"""

from sklearn.model_selection import train_test_split
train_features, val_features, train_labels, val_labels = train_test_split( features, labels, test_size=0.2 )
train_features = tf.convert_to_tensor( train_features )
val_features = tf.convert_to_tensor( val_features )

# If you use convolutional layers, you must add a dimension to the features
# If you don't use convolution layers, you should comment these lines out
train_features = tf.convert_to_tensor( np.reshape( train_features, ((train_features.shape[0], train_features.shape[1], 1))) )
val_features = tf.convert_to_tensor( np.reshape( val_features, ((val_features.shape[0], val_features.shape[1], 1))) )

"""## Convert the output categories from a single integer value into one-hot vectors
The result of the IntegerLookup layer will be each of the classifications (0,1,2,3) plus a -1 which indicates 'Unknown'

Encode both the training and testing labels as one-hot vectors

*You don't necessarily have to do this step if you don't want to use one-hot vectors for output.*
"""

# Set up one hot vector for output
categorizer = keras.layers.StringLookup( output_mode = 'one_hot' )

categorizer.adapt( train_labels )
print( categorizer.get_vocabulary() )

# Now encode both the training and testing labels as one-hot vectors
encoded_labels = categorizer( train_labels )
encoded_val_labels = categorizer( val_labels )

"""## Build and compile the model
* To do
"""

from keras import layers

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape = (2000,1)))
model.add(tf.keras.layers.Conv1D(filters = 16, kernel_size = 3, activation = 'relu'))
model.add(tf.keras.layers.MaxPool1D(pool_size = 2))
model.add(tf.keras.layers.Conv1D(filters = 16, kernel_size = 3, activation = 'relu'))
model.add(tf.keras.layers.MaxPool1D(pool_size = 2))
model.add(tf.keras.layers.Conv1D(filters = 16, kernel_size = 3, activation = 'relu'))
model.add(tf.keras.layers.MaxPool1D(pool_size = 2))
model.add(tf.keras.layers.Conv1D(filters = 16, kernel_size = 3, activation = 'relu'))
model.add(tf.keras.layers.MaxPool1D(pool_size = 2))
model.add(tf.keras.layers.Conv1D(filters = 8, kernel_size = 3, activation = 'relu'))
model.add(tf.keras.layers.MaxPool1D(pool_size = 2))
model.add(tf.keras.layers.Conv1D(filters = 8, kernel_size = 3, activation = 'relu'))
model.add(tf.keras.layers.MaxPool1D(pool_size = 2))
model.add(tf.keras.layers.Conv1D(filters = 8, kernel_size = 3, activation = 'relu'))
model.add(tf.keras.layers.MaxPool1D(pool_size = 2))
model.add(tf.keras.layers.Conv1D(filters = 8, kernel_size = 3, activation = 'relu'))
model.add(tf.keras.layers.MaxPool1D(pool_size = 2))

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(units = 512, activation = "relu"))
model.add(layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(units = 256, activation = 'relu'))
model.add(layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))
model.add(layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(units = 64, activation = 'relu'))
model.add(layers.Dense(units = 5, activation = 'softmax'))

#Compile the model
my_optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.0008)
my_loss_function = tf.keras.losses.CategoricalCrossentropy()
my_metrics = [tf.keras.metrics.CategoricalAccuracy()]
model.compile(optimizer = my_optimizer, loss = my_loss_function, metrics = my_metrics)

#Visualize the model
model.summary
tf.keras.utils.plot_model(model,show_shapes = True, expand_nested = True)

"""## Train the model
* To do
"""

my_epochs = 50
my_batch_size = 32
history =  model.fit(train_features, encoded_labels, epochs = my_epochs, batch_size =  my_batch_size, validation_data = (val_features, encoded_val_labels))

"""## Plotting function
Same one we have been using before
"""

##########
# Plot the loss as a function of epoch.  Can use this plot to determine how the
# training went.
def plot_loss( epochs, loss, label, val=None ):

# Make a figure and add axes labels
    plt.figure()
    plt.xlabel( 'Epoch' )
    plt.ylabel( label )

# Just plot the loss vs the epoch number
    plt.plot( epochs, loss )
    if val != None:
      plt.plot( epochs, val )
    plt.show()
#
# End of plot_loss function
##########

"""## Show the model
Use the plot_model() function to show a graphical representation of the model for the assignment
"""

model.summary()
keras.utils.plot_model( model, show_shapes=True, expand_nested=True )
plot_loss(history.epoch,history.history['loss'], 'Loss', history.history['val_loss'])

"""## Save the model
Use this command to save evetything about your model (the architecture, the trained weights) in HDF5 format -- you can save this anywhere, just make sure the extension is .h5 and it will save in the correct format.

This is useful because we don't want to have to rebuild the model and retrain the model every time we want to use it - especially if the model took a really long time to train.
"""

from google.colab import drive
drive.mount('/content/gdrive')

output_fname = '/content/gdrive/MyDrive/Colab Notebooks/my_ECG_model.h5'
model.save( output_fname )

"""## Use the saved model
Instructor will evaluate a set of test data by loading in your saved model

"""

model_fname = output_fname
saved_model = keras.models.load_model( model_fname )

# Load in test data here
# test_features = []
# encoded_test_labels = []

encoded_test_labels = categorizer( test_labels )
results = saved_model.evaluate( np.asarray( test_features), np.asarray( encoded_test_labels) )